{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ME18B156 - Assignment 3 - Transliteration"
      ],
      "metadata": {
        "id": "IUOzMgZn4ApC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO9SivEHp9C8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Mukesh-V/Acads.git\n",
        "%cd /content/Acads/DL/AS-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xz2KFaDrY_0"
      },
      "outputs": [],
      "source": [
        "!pip install wandb pytorch-lightning Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlXXVL4BrhSD"
      },
      "outputs": [],
      "source": [
        "!wandb online\n",
        "%env WANDB_API_KEY=77d02595152611dfbdff0cd06134f43db0cf56f3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py has two arguments : learning rate and teacher-forcing. Both should lie between 0 and 1. Keeping the teacher-forcing higher reduces its effect (it is applied lesser times)"
      ],
      "metadata": {
        "id": "uxZpgfQB4Xud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AftrLvfEq_4L"
      },
      "outputs": [],
      "source": [
        "!python3 main.py --lr=0.01 --tf=0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded from wandb's best run\n",
        "no_att_best_run = 'model-l1i1t17s:v10'\n",
        "no_att_config = {\n",
        "    'model': 'no-attention',\n",
        "    'drop': 0.1,\n",
        "    'embedding': 16,\n",
        "    'unit': 'gru',\n",
        "    'hidden': 256,\n",
        "    'layers': 3,\n",
        "    'epochs': 20\n",
        "}\n",
        "\n",
        "att_best_run = 'model-9y3fvg3a:v18'\n",
        "att_config = {\n",
        "    'model': 'attention',\n",
        "    'drop': 0.05,\n",
        "    'embedding': 16,\n",
        "    'unit': 'gru',\n",
        "    'hidden': 64,\n",
        "    'layers': 3,\n",
        "    'epochs': 30\n",
        "}"
      ],
      "metadata": {
        "id": "UN1SXo-zZUwK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "%env WANDB_API_KEY=77d02595152611dfbdff0cd06134f43db0cf56f3\n",
        "\n",
        "run = wandb.init()\n",
        "no_attention_artifact = run.use_artifact('muttaborota/FundDL-AS3/' + no_att_best_run, type='model')\n",
        "no_attention_artifact_dir = no_attention_artifact.download()\n",
        "\n",
        "attention_artifact = run.use_artifact('muttaborota/FundDL-AS3/' + att_best_run, type='model')\n",
        "attention_artifact_dir = attention_artifact.download()\n",
        "wandb.run.finish()"
      ],
      "metadata": {
        "id": "_bGg7N1Rfoa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data import TransliterationDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "testloader = DataLoader(TransliterationDataset('test'), batch_size, num_workers=2)"
      ],
      "metadata": {
        "id": "IsU4XUjYdWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model_no_att import Transliterator\n",
        "from model_att import AttentionTransliterator\n",
        "\n",
        "maps = testloader.dataset.maps\n",
        "\n",
        "# Loading the models\n",
        "class Struct:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "no_attention_struct = Struct(**no_att_config)\n",
        "attention_struct = Struct(**att_config)\n",
        "\n",
        "no_attention_model = Transliterator.load_from_checkpoint('./artifacts/' + no_att_best_run + '/model.ckpt', config=no_attention_struct, maps=maps, map_location=torch.device('cpu'), strict=False)\n",
        "attention_model = AttentionTransliterator.load_from_checkpoint('./artifacts/' + att_best_run + '/model.ckpt', config=attention_struct, maps=maps, map_location=torch.device('cpu'), strict=False)"
      ],
      "metadata": {
        "id": "nY5gB1rucHqp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Accuracy"
      ],
      "metadata": {
        "id": "r6Syl41I5EKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C4Y9Esw_5EIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Softmax\n",
        "\n",
        "maps = testloader.dataset.maps\n",
        "crct = 0\n",
        "for i, batch in enumerate(testloader):\n",
        "  x, y = batch\n",
        "  preds = torch.argmax(Softmax(dim=2)(no_attention_model(x)), dim=2)       \n",
        "  df = open('predictions_vanilla.txt', 'w') \n",
        "\n",
        "  for input, pred, truth in zip(x, preds, y):\n",
        "    input_word = ''.join([maps['i2ic'][i.item()] for i in input]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "    pred_word = ''.join([maps['i2oc'][i.item()] for i in pred]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "    truth_word = ''.join([maps['i2oc'][i.item()] for i in truth]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "\n",
        "    if(pred_word == truth_word): crct += 1\n",
        "    df.write('{} {} {} \\n'.format(input_word, truth_word, pred_word))\n",
        "  df.close()\n",
        "\n",
        "print('Accuracy : ', crct/(len(x)*len(testloader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkbkwcsQd6mR",
        "outputId": "26cbbf25-aebe-4258-d857-3858c3bc9c1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  0.15826810176125244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Softmax\n",
        "\n",
        "maps = testloader.dataset.maps\n",
        "crct = 0\n",
        "for i, batch in enumerate(testloader):\n",
        "  x, y = batch\n",
        "  preds = torch.argmax(Softmax(dim=2)(attention_model(x)), dim=2)  \n",
        "  df = open('predictions_attention.txt', 'w') \n",
        "\n",
        "  for input, pred, truth in zip(x, preds, y):\n",
        "    input_word = ''.join([maps['i2ic'][i.item()] for i in input]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "    pred_word = ''.join([maps['i2oc'][i.item()] for i in pred]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "    truth_word = ''.join([maps['i2oc'][i.item()] for i in truth]).replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
        "\n",
        "    if(pred_word == truth_word): crct += 1\n",
        "    df.write('{} {} {} \\n'.format(input_word, truth_word, pred_word))\n",
        "  df.close()\n",
        "\n",
        "print('Accuracy : ', crct/(len(x)*len(testloader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi6Ex06CnDvE",
        "outputId": "beda9337-0467-4627-bcd9-5af239c3ea13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  0.14554794520547945\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}